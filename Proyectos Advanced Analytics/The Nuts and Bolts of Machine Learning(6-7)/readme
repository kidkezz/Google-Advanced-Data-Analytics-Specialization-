Machine Learning Models and Feature Engineering
This repository contains a series of notebooks covering feature engineering techniques and the implementation of various machine learning models. Below is a detailed description of each notebook in execution order.

1. Perform Feature Engineering
This notebook covers essential feature engineering techniques for data preparation before training machine learning models. It includes:

Handling missing values and outliers.
Transformations of categorical variables.
Creating new features from existing data.
Scaling and normalizing numerical variables.

2. Build a Naive Bayes Model
Implementation of a classification model based on Naive Bayes. It includes:

Theoretical explanation of the algorithm.
Data preprocessing and splitting into training and testing sets.
Model implementation using sklearn.naive_bayes.
Model evaluation with metrics such as precision, recall, and confusion matrix.

3. Build a K-means Model
Implementation of the K-means clustering algorithm. It includes:

Explanation of the algorithm and its application.
Using the elbow method to determine the optimal number of clusters.
Training the model with sklearn.cluster.KMeans.
Visualization of clusters in the feature space.

4. Build a Decision Tree Model
Implementation of a decision tree for classification. It includes:

Theoretical explanation of the algorithm.
Training the model with sklearn.tree.DecisionTreeClassifier.
Hyperparameter tuning to prevent overfitting.
Model evaluation using accuracy and confusion matrix.

5. Build a Random Forest Model
Implementation of a Random Forest model for classification. It includes:

Comparison with individual decision trees.
Training with sklearn.ensemble.RandomForestClassifier.
Hyperparameter tuning with cross-validation.
Model evaluation using performance metrics.

6. Build an XGBoost Model
Implementation of the XGBoost model for classification or regression. It includes:

Explanation of the algorithm and its advantages over other models.
Implementation using xgboost.XGBClassifier.
Hyperparameter tuning through grid search.
Model evaluation and comparison with other approaches.
